# 正向传播、反向传播
## 1 正向传播
正向传播是指对神经⽹络沿着从输⼊层到输出层的顺序，依次计算并存储模型的中间变量（包括输出）。为简单起⻅，假设输⼊是⼀个特征为x∈Rd的样本，且不考虑偏差项，那么中间变量

<div align=center>
    ![image](https://raw.githubusercontent.com/zhiyuanz-hub/public-picture/master/20-1.png)
</div> 

其中W(1)∈Rh×d是隐藏层的权重参数。把中间变量z∈Rh输⼊按元素运算的激活函数ϕ后，将得到向量⻓度为h的隐藏层变量

<div align=center>
    ![image](https://raw.githubusercontent.com/zhiyuanz-hub/public-picture/master/20-2.png)
</div> 

隐藏层变量h也是⼀个中间变量。假设输出层参数只有权重W(2)∈Rq×h，可以得到向量⻓度为q的输出层变量

<div align=center>
    ![image](https://raw.githubusercontent.com/zhiyuanz-hub/public-picture/master/20-3.png)
</div> 

假设损失函数为ℓ，且样本标签为y，可以计算出单个数据样本的损失项

<div align=center>
    ![image](https://raw.githubusercontent.com/zhiyuanz-hub/public-picture/master/20-4.png)
</div> 

根据L2范数正则化的定义，给定超参数λ，正则化项即

<div align=center>
    ![image](https://raw.githubusercontent.com/zhiyuanz-hub/public-picture/master/20-5.png)
</div> 

其中矩阵的Frobenius范数等价于将矩阵变平为向量后计算L2范数。最终，模型在给定的数据样本上带正则化的损失为

<div align=center>
    J = L + s
</div>

将J称为有关给定数据样本的⽬标函数，并在以下的讨论中简称⽬标函数。
## 2 正向传播的计算图
通常绘制计算图来可视化运算符和变量在计算中的依赖关系。下图绘制了本节中样例模型正向传播的计算图，其中左下⻆是输⼊，右上⻆是输出。可以看到，图中箭头⽅向⼤多是向右和向上，其中⽅框代表变量，圆圈代表运算符，箭头表⽰从输⼊到输出之间的依赖关系。

<div align=center>
    ![image](https://raw.githubusercontent.com/zhiyuanz-hub/public-picture/master/20-6.jpg)
</div> 
## 3 反向传播
反向传播指的是计算神经⽹络参数梯度的⽅法。总的来说，反向传播依据微积分中的链式法则，沿着从输出层到输⼊层的顺序，依次计算并存储⽬标函数有关神经⽹络各层的中间变量以及参数的梯度。对输⼊或输出X; Y; Z为任意形状张量的函数Y = f(X)和Z = g(Y)，通过链式法则，有

<div align=center>
    ![image](https://raw.githubusercontent.com/zhiyuanz-hub/public-picture/master/20-7.png)
</div> 

其中prod运算符将根据两个输⼊的形状，在必要的操作（如转置和互换输⼊位置）后对两个输⼊
做乘法。

最后对本节进行一个小结：
+ 正向传播沿着从输⼊层到输出层的顺序，依次计算并存储神经⽹络的中间变量。
+ 反向传播沿着从输出层到输⼊层的顺序，依次计算并存储神经⽹络中间变量和参数的梯度。