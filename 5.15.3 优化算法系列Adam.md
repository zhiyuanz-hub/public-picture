# Adam
## 1 Adam 算法概述
Adam源于适应性矩估计（adaptive moment estimation），Adam算法和传统的随机梯度下降
不同。随机梯度下降保持单一的学习率（即alpha）更新所有的权重，学习率在训练过程中并不会
改变。而Adam通过计算梯度的一阶矩估计和二阶矩估计而为不同的参数设计独立的自适应性学习率。 
Adam算法的提出者描述其为两种随机梯度下降扩展式的优点集合，即：
1. 适应性梯度算法（AdaGrad）为每一个参数保留一个学习率以提升在稀疏梯度（即自然语言和计算机视觉问题）上的性能。
2.均方根传播（RMSProp）基于权重梯度最近量级的均值为每一个参数适应性地保留学习率。这意味着算法在非稳态和在线问题上有很有优秀的性能。

<div align=center>
    ![image](https://raw.githubusercontent.com/zhiyuanz-hub/public-picture/master/6-1.jpg)
</div>

+ Momentum项：我们把这个人从平地上放到了一个斜坡上，只要他往下坡的方向走一点点，由于向
下的惯性，他不自觉地就一直往下走，走的弯路也变少了。这就是Momentum参数更新。
+ RMSProp项：他的作用和momentum类似，不过不是给喝醉酒的人安排另一个下坡，而是给他一双
不好走路的鞋子，使得他一摇晃着走路就脚疼，鞋子成为了走弯路的阻力，逼着他往前直着走。

计算s时有momentum下坡的属性，计算r时有adagrad阻力的属性，然后再更新参数时把s和r都考虑
进去。实验证明，大多数时候，使用adam都能又快又好的达到目标，迅速收敛。所以说，在加速神经
网络训练的时候，一个下坡，一双破鞋子，功不可没。

## 2 Adam 优势
+ 直截了当地实现
+ 高效的计算
+ 所需内存少
+ 梯度对角缩放的不变性
+ 适合解决含大规模数据和参数的优化问题
+ 适用于非稳态（non-stationary）目标
+ 适用于解决包含很高噪声或稀疏梯度的问题
+ 超参数可以很直观地解释，并且基本上只需极少量的调参

## 3 Adam 小结
1. Adam是一种在深度学习模型中用来替代随机梯度下降的优化算法。
2. Adam结合了 AdaGrad 和 RMSProp 算法最优的性能，它还是能提供解决稀疏梯度和噪声问题的
优化方法。
3. Adam的调参相对简单，默认参数就可以处理绝大部分的问题。

