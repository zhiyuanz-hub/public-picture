# RMSProp(均方根)
## 1 RMSProp方法概述
RMSProp算法的全称叫Root Mean Square Prop，是Geoffrey E. Hinton在Coursera课程中提出的一种优化算法，在之前讲到的动量优化算法中，虽然初步解决了优化中摆动幅度大的问题。所谓的摆动幅度就是在优化中经过更新之后参数的变化范围，通过一个图进行展示

<div align=center>
    ![image](https://raw.githubusercontent.com/zhiyuanz-hub/public-picture/master/8-1.jpg)
</div>

蓝色的为动量优化算法所走的路线，绿色的为RMSProp优化算法所走的路线。 为了进一步优化损失函数在更新中存在摆动幅度过大的问题，并且进一步加快函数的收敛速度，RMSProp算法对权重W和偏置b的梯度使用了微分平方加权平均数。

其中，假设在第 t 轮迭代过程中，各个公式如下所示：

<div align=center>
    ![image](https://raw.githubusercontent.com/zhiyuanz-hub/public-picture/master/8-2.jpg)
</div>

算法的主要思想就用上面的公式表达完毕了。在上面的公式中sdw和sdb分别是损失函数在前t−1轮迭代过程中累积的梯度梯度动量，β是梯度累积的一个指数。所不同的是，RMSProp算法对梯度计算了微分平方加权平均数。这种做法有利于消除了摆动幅度大的方向，用来修正摆动幅度，使得各个维度的摆动幅度都较小。另一方面也使得网络函数收敛更快。（比如当dW或者db中有一个值比较大的时候，那么我们在更新权重或者偏置的时候除以它之前累积的梯度的平方根，这样就可以使得更新幅度变小）。为了防止分母为零，使用了一个很小的数值ϵ来进行平滑，一般取值为10−8。

## 2 算法概述
<div align=center>
    ![image](https://raw.githubusercontent.com/zhiyuanz-hub/public-picture/master/8-3.jpg)
</div>

## 3 RMSProP小结
1. 鉴于神经网络都是非凸条件下的，RMSProp在非凸条件下结果更好，改变梯度累积为指数衰减的移动平均以丢弃遥远的过去历史。
2. 经验上，RMSProp被证明有效且是实用的深度学习网络优化算法。
