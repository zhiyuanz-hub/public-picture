# 词向量介绍
简而言之，词向量技是将词转化成为稠密向量，并且对于相似的词，其对应的词向量也相近。
## 1 词的表示
在自然语言处理任务中，首先需要考虑词如何在计算机中表示。通常，有两种表示方式：离散表示和分布式表示。
### 1.1 离散表示
传统的基于规则或基于统计的自然语义处理方法将单词看作一个原子符号被称作one-hot representation。离散表示把每个词表示为一个长向量。这个向量的维度是词表大小，向量中只有一个维度的值为1，其余维度为0，这个维度就代表了当前的词。 例如：苹果[0，0，0，1，0，0，0，0，0，……]，离散表示相当于给每个词分配一个id，这就导致这种表示方式不能展示词与词之间的关系。另外，离散表示将会导致特征空间非常大，但也带来一个好处，就是在高维空间中，很多应用任务线性可分。
### 1.2 分布式表示
分布式表示指的是将词转化成一种分布式表示，又称词向量。分布式表示将词表示成一个定长的连续的稠密向量。分布式表示优点：(1)词之间存在相似关系：是词之间存在“距离”概念，这对很多自然语言处理的任务非常有帮助。 (2)包含更多信息：词向量能够包含更多信息，并且每一维都有特定的含义。在采用one-hot特征时，可以对特征向量进行删减，词向量则不能。
## 2 词向量生成
### 2.1 基于统计方法
有两种：
1. 共现矩阵：通过统计一个事先指定大小的窗口内的词共现次数，以词周边的共现词的次数做为当前词的向量。
2. SVD(奇异值分解)：进行SVD分解，得到矩阵正交矩阵U，对U进行归一化得到矩阵，SVD得到了词的稠密矩阵，该矩阵具有很多良好的性质：语义相近的词在向量空间相近，甚至可以一定程度反映词间的线性关系。
### 2.2 语言模型
语言模型生成词向量是通过训练神经网络语言模型，词向量做为语言模型的附带产出。神经网络语言模型背后的基本思想是对出现在上下文环境里的词进行预测，这种对上下文环境的预测本质上也是一种对共现统计特征的学习。较为著名的采用神经网络语言模型Skip-gram、CBOW、LBL、NNLM、C&W、GloVe等。

