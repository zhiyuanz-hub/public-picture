# 词嵌入
⾃然语⾔是⼀套⽤来表达含义的复杂系统。在这套系统中，词是表义的基本单元。顾名思义，词向量是⽤来表⽰词的向量，也可被认为是词的特征向量或表征。把词映射为实数域向量的技术也叫词嵌⼊（word embedding）。近年来，词嵌⼊已逐渐成为⾃然语⾔处理的基础知识。

## 1 为何不采⽤one-hot向量

虽然one-hot词向量构造起来很容易，但通常并不是⼀个好选择。⼀个主要的原因是，one-hot词
向量⽆法准确表达不同词之间的相似度，如常常使⽤的余弦相似度。对于向量x,y∈Rd，它们的余弦相似度是它们之间夹⻆的余弦值

<div align=center>
    ![image](https://raw.githubusercontent.com/zhiyuanz-hub/public-picture/master/24-1.png)
</div> 

由于任何两个不同词的one-hot向量的余弦相似度都为0，多个不同词之间的相似度难以通过one-hot向量准确地体现出来。

word2vec⼯具的提出正是为了解决上⾯这个问题。它将每个词表⽰成⼀个定⻓的向量，并使得这些向量能较好地表达不同词之间的相似和类⽐关系。word2vec⼯具包含了两个模型，即跳字模型（skip-gram）和连续词袋模型（continuous bag of words，CBOW）。接下来让分别介绍这两个模型以及它们的训练⽅法。

## 2 跳字模型

跳字模型假设基于某个词来⽣成它在⽂本序列周围的词。举个例⼦，假设⽂本序列是“the”“man”“loves”“his”“son”。以“loves”作为中⼼词，设背景窗口⼤小为2。如第一个图所⽰，跳字模型所关⼼的是，给定中⼼词“loves”，⽣成与它距离不超过2个词的背景词“the”“man”“his”“son”的条件概率，即

<div align=center>
    ![image](https://raw.githubusercontent.com/zhiyuanz-hub/public-picture/master/24-2.png)
</div>

假设给定中⼼词的情况下，背景词的⽣成是相互独⽴的，那么上式可以改写成

<div align=center>
    ![image](https://raw.githubusercontent.com/zhiyuanz-hub/public-picture/master/24-3.png)
</div>

<div align=center>
    ![image](https://raw.githubusercontent.com/zhiyuanz-hub/public-picture/master/24-4.png)
</div>

在跳字模型中，每个词被表⽰成两个d维向量，⽤来计算条件概率。假设这个词在词典中索引为i，
当它为中⼼词时向量表⽰为vi∈Rd，而为背景词时向量表⽰为ui∈Rd。设中⼼词wc在词典中索引为c，背景词wo在词典中索引为o，给定中⼼词⽣成背景词的条件概率可以通过对向量内积做sofmax运算而得到：

<div align=center>
    ![image](https://raw.githubusercontent.com/zhiyuanz-hub/public-picture/master/24-5.png)
</div>

其中词典索引集V={0,1,...,|V|−1}。假设给定⼀个⻓度为T的⽂本序列，设时间步t的词为w(t)。
假设给定中⼼词的情况下背景词的⽣成相互独⽴，当背景窗口⼤小为m时，跳字模型的似然函数
即给定任⼀中⼼词⽣成所有背景词的概率

<div align=center>
    ![image](https://raw.githubusercontent.com/zhiyuanz-hub/public-picture/master/24-6.png)
</div>

这⾥小于1和⼤于T的时间步可以忽略。

**训练跳字模型**

跳字模型的参数是每个词所对应的中⼼词向量和背景词向量。训练中通过最⼤化似然函数来学习模型参数，即最⼤似然估计。这等价于最小化以下损失函数：

<div align=center>
    ![image](https://raw.githubusercontent.com/zhiyuanz-hub/public-picture/master/24-7.png)
</div>

如果使⽤随机梯度下降，那么在每⼀次迭代⾥随机采样⼀个较短的⼦序列来计算有关该⼦序列的损失，然后计算梯度来更新模型参数。梯度计算的关键是条件概率的对数有关中⼼词向量和背景词向量的梯度。根据定义，⾸先看到

<div align=center>
    ![image](https://raw.githubusercontent.com/zhiyuanz-hub/public-picture/master/24-8.png)
</div>

通过微分，可以得到上式中vc的梯度

<div align=center>
    ![image](https://raw.githubusercontent.com/zhiyuanz-hub/public-picture/master/24-9.jpg)
</div>

它的计算需要词典中所有词以wc为中⼼词的条件概率。有关其他词向量的梯度同理可得。

训练结束后，对于词典中的任⼀索引为i的词，均得到该词作为中⼼词和背景词的两组词向量vi和ui。在⾃然语⾔处理应⽤中，⼀般使⽤跳字模型的中⼼词向量作为词的表征向量。

## 3 连续词袋模型

连续词袋模型与跳字模型类似。与跳字模型最⼤的不同在于，连续词袋模型假设基于某中⼼词在⽂本序列前后的背景词来⽣成该中⼼词。在同样的⽂本序列“the”“man”“loves”“his”“son”⾥，以“loves”作为中⼼词，且背景窗口⼤小为2时，连续词袋模型关⼼的是，给定背景词“the”“man”“his”“son”⽣成中⼼词“loves”的条件概率（如第二幅图所示），也就是

<div align=center>
    ![image](https://raw.githubusercontent.com/zhiyuanz-hub/public-picture/master/24-10.png)
</div>

<div align=center>
    ![image](https://raw.githubusercontent.com/zhiyuanz-hub/public-picture/master/24-11.png)
</div>

因为连续词袋模型的背景词有多个，将这些背景词向量取平均，然后使⽤和跳字模型⼀样的⽅法来计算条件概率。设vi∈Rd和ui∈Rd分别表⽰词典中索引为i的词作为背景词和中⼼词的向量（注意符号的含义与跳字模型中的相反）。设中⼼词wc在词典中索引为c，背景词wo1,...,wo2m在词典中索引为o1,...,o2m，那么给定背景词⽣成中⼼词的条件概率

<div align=center>
    ![image](https://raw.githubusercontent.com/zhiyuanz-hub/public-picture/master/24-12.png)
</div>

为了让符号更加简单，记Wo={wo1,...,wo2m}，且v¯o=(vo1+...+vo2m) /(2m)，那么上式可以简写成

<div align=center>
    ![image](https://raw.githubusercontent.com/zhiyuanz-hub/public-picture/master/24-13.png)
</div>

给定⼀个⻓度为T的⽂本序列，设时间步t的词为w(t)，背景窗口⼤小为m。连续词袋模型的似然
函数是由背景词⽣成任⼀中⼼词的概率

<div align=center>
    ![image](https://raw.githubusercontent.com/zhiyuanz-hub/public-picture/master/24-14.png)
</div>

**训练连续词袋模型**

训练连续词袋模型同训练跳字模型基本⼀致。连续词袋模型的最⼤似然估计等价于最小化损失函
数

<div align=center>
    ![image](https://raw.githubusercontent.com/zhiyuanz-hub/public-picture/master/24-15.png)
</div>

注意到

<div align=center>
    ![image](https://raw.githubusercontent.com/zhiyuanz-hub/public-picture/master/24-16.png)
</div>

通过微分，可以计算出上式中条件概率的对数有关任⼀背景词向量voi（i=1,...,2m）的梯度

<div align=center>
    ![image](https://raw.githubusercontent.com/zhiyuanz-hub/public-picture/master/24-17.png)
</div>

有关其他词向量的梯度同理可得。同跳字模型不⼀样的⼀点在于，⼀般使⽤连续词袋模型的
背景词向量作为词的表征向量。

最后对这一小节做一个小结：
+ 词向量是⽤来表⽰词的向量。把词映射为实数域向量的技术也叫词嵌⼊。
+ word2vec包含跳字模型和连续词袋模型。跳字模型假设基于中⼼词来⽣成背景词。连续词
袋模型假设基于背景词来⽣成中⼼词。

