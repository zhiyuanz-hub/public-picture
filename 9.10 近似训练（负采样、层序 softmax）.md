# 近似训练（负采样、层序 softmax）
不论是跳字模型还是连续词袋模型，由于条件概率使用了softmax运算，每一步的梯度计算都包含词典大小数目的项的累加。对于含几十万或上百万词的较大词典，每次的梯度计算开销可能过大。为了降低该计算复杂度，将介绍两种近似训练方法，即负采样（negative sampling）或层序softmax（hierarchical softmax）。由于跳字模型和连续词袋模型类似，仅以跳字模型为例介绍这两种方法。
## 1 负采样
负采样修改了原来的目标函数。给定中心词wc的一个背景窗口，把背景词wo出现在该背景窗口看作一个事件，并将该事件的概率计算为

<div align=center>
    ![image](https://raw.githubusercontent.com/zhiyuanz-hub/public-picture/master/25-1.png)
</div>

其中的 σ 函数与sigmoid激活函数的定义相同：

<div align=center>
    ![image](https://raw.githubusercontent.com/zhiyuanz-hub/public-picture/master/25-2.png)
</div>

先考虑最大化文本序列中所有该事件的联合概率来训练词向量。具体来说，给定一个长度为T的文本序列，设时间步t的词为w(t)且背景窗口大小为m，考虑最大化联合概率

<div align=center>
    ![image](https://raw.githubusercontent.com/zhiyuanz-hub/public-picture/master/25-3.png)
</div>

然而，以上模型中包含的事件仅考虑了正类样本。这导致当所有词向量相等且值为⽆穷⼤时，以上的联合概率才被最⼤化为1。很明显，这样的词向量毫⽆意义。负采样通过采样并添加负类样本使⽬标函数更有意义。设背景词wo出现在中⼼词wc的⼀个背景窗口为事件P，根据分布P(w)采样K个未出现在
该背景窗口中的词，即噪声词。设噪声词wk(k=1,...,K)不出现在中⼼词wc的该背景窗口为事件Nk。假设同时含有正类样本和负类样本的事件P,N1,...,NK相互独⽴，负采样将以上需要最⼤化的仅考虑正类样本的联合概率改写为

<div align=center>
    ![image](https://raw.githubusercontent.com/zhiyuanz-hub/public-picture/master/25-4.png)
</div>

其中条件概率被近似表⽰为

<div align=center>
    ![image](https://raw.githubusercontent.com/zhiyuanz-hub/public-picture/master/25-5.png)
</div>

设⽂本序列中时间步t的词w(t)在词典中的索引为it，噪声词wk在词典中的索引为hk。有关以上条件概率的对数损失为

<div align=center>
    ![image](https://raw.githubusercontent.com/zhiyuanz-hub/public-picture/master/25-6.jpg)
</div>

现在，训练中每⼀步的梯度计算开销不再与词典⼤小相关，而与K线性相关。当K取较小的常数时，负采样在每⼀步的梯度计算开销较小。

## 2 层序sofmax
层序sofmax是另⼀种近似训练法。它使⽤了⼆叉树这⼀数据结构，树的每个叶结点代表词典V中
的每个词。

<div align=center>
    ![image](https://raw.githubusercontent.com/zhiyuanz-hub/public-picture/master/25-7.jpg)
</div>

假设L(w)为从⼆叉树的根结点到词w的叶结点的路径（包括根结点和叶结点）上的结点数。设n(w;j)为该路径上第j个结点，并设该结点的背景词向量为un(w;j)。以上图为例，L(w3)=4。层序sofmax将跳字模型中的条件概率近似表⽰为

<div align=center>
    ![image](https://raw.githubusercontent.com/zhiyuanz-hub/public-picture/master/25-8.png)
</div>

其中σ函数与sigmoid激活函数的定义相同，lefChild(n)是结点n的左⼦结点：如果判断x为真， [[x]] = 1；反之[[x]] = −1。让我们计算图10.3中给定词wc⽣成词w3的条件概率。需要将wc的词向量vc和根结点到w3路径上的⾮叶结点向量⼀⼀求内积。由于在⼆叉树中由根结点到叶结点w3的路径上需要向左、向右再向左地遍历（上图中加粗的路径），得到

<div align=center>
    ![image](https://raw.githubusercontent.com/zhiyuanz-hub/public-picture/master/25-9.png)
</div>

由于σ(x)+σ(−x)=1，给定中⼼词wc⽣成词典V中任⼀词的条件概率之和为1这⼀条件也将满⾜：

<div align=center>
    ![image](https://raw.githubusercontent.com/zhiyuanz-hub/public-picture/master/25-10.png)
</div>

此外，由于L(wo)−1的数量级为O(log2|V|)，当词典V很⼤时，层序sofmax在训练中每⼀步的梯
度计算开销相较未使⽤近似训练时⼤幅降低。

最后对这一小节做一个小结：
+ 负采样通过考虑同时含有正类样本和负类样本的相互独⽴事件来构造损失函数。其训练中
每⼀步的梯度计算开销与采样的噪声词的个数线性相关。
+ 层序sofmax使⽤了⼆叉树，并根据根结点到叶结点的路径来构造损失函数。其训练中每⼀
步的梯度计算开销与词典⼤小的对数相关。

