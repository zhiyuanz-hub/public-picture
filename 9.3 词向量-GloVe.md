# 词向量-GloVe
## 1 什么是GloVe？
GloVe的全称叫Global Vectors for Word Representation，它是一个基于全局词频统计（count-based & overall statistics）的词表征（word representation）工具，它可以把一个单词表达成一个由实数组成的向量，这些向量捕捉到了单词之间一些语义特性，比如相似性（similarity）、类比性（analogy）等。通过对向量的运算，比如欧几里得距离或者cosine相似度，可以计算出两个单词之间的语义相似性。

## 2 Glove是如何实现的？
GloVe的实现分为以下三步：

第一步：根据语料库（corpus）构建一个共现矩阵（Co-ocurrence Matrix）X，矩阵中的每一个元素Xij代表单词i和上下文单词j在特定大小的上下文窗口（context window）内共同出现的次数。一般而言，这个次数的最小单位是1，但是GloVe不这么认为：它根据两个单词在上下文窗口的距离d，提出了一个衰减函数（decreasing weighting）：decay=1/d用于计算权重，也就是说距离越远的两个单词所占总计数（total count）的权重越小。

第二步：构建词向量（Word Vector）和共现矩阵（Co-ocurrence Matrix）之间的近似关系，论文的作者提出以下的公式可以近似地表达两者之间的关系：

<div align=center>
    ![image](https://raw.githubusercontent.com/zhiyuanz-hub/public-picture/master/22-1.png)
</div>

其中，wTi和w^j是我们最终要求解的词向量；bi和b^j分别是两个词向量的bias term。当然你对这个公式一定有非常多的疑问，比如它到底是怎么来的，为什么要使用这个公式，为什么要构造两个词向量wTi和w^j？下文会详细介绍:

第三步：有了公式1之后我们就可以构造它的loss function了：

<div align=center>
    ![image](https://raw.githubusercontent.com/zhiyuanz-hub/public-picture/master/22-2.png)
</div>

这个loss function的基本形式就是最简单的mean square loss，只不过在此基础上加了一个权重函数f(Xij)，那么这个函数起了什么作用，为什么要添加这个函数呢？我们知道在一个语料库中，肯定存在很多单词他们在一起出现的次数是很多的（frequent co-occurrences），那么希望：
1. 这些单词的权重要大于那些很少在一起出现的单词（rare co-occurrences），所以这个函数要是非递减函数（non-decreasing）；
2. 但也不希望这个权重过大（overweighted），当到达一定程度之后应该不再增加；
3. 如果两个单词没有在一起出现，也就是Xij=0，那么他们应该不参与到loss function的计算当中去，也就是f(x)要满足f(0)=0。

满足以上两个条件的函数有很多，作者采用了如下形式的分段函数：

<div align=center>
    ![image](https://raw.githubusercontent.com/zhiyuanz-hub/public-picture/master/22-3.png)
</div>

这个函数图像如下所示：

<div align=center>
    ![image](https://raw.githubusercontent.com/zhiyuanz-hub/public-picture/master/22-4.png)
</div>

这篇论文中的所有实验，α的取值都是0.75，而xmax取值都是100。以上就是GloVe的实现细节，那么GloVe是如何训练的呢？
## 3 GloVe是如何训练的？

虽然很多人声称GloVe是一种无监督（unsupervised learing）的学习方式（因为它确实不需要人工标注label），但其实它还是有label的，这个label就是公式2中的log(Xij)，而公式2中的向量w和w^就是要不断更新/学习的参数，所以本质上它的训练方式跟监督学习的训练方法没什么不一样，都是基于梯度下降的。具体地，这篇论文里的实验是这么做的：采用了AdaGrad的梯度下降算法，对矩阵X中的所有非零元素进行随机采样，学习曲率（learning rate）设为0.05，在vector size小于300的情况下迭代了50次，其他大小的vectors上迭代了100次，直至收敛。最终学习得到的是两个vector是w和w^，因为X是对称的（symmetric），所以从原理上讲w和w^是也是对称的，他们唯一的区别是初始化的值不一样，而导致最终的值不一样。所以这两者其实是等价的，都可以当成最终的结果来使用。但是为了提高鲁棒性，我们最终会选择两者之和
w+w^作为最终的vector（两者的初始化不同相当于加了不同的随机噪声，所以能提高鲁棒性）。